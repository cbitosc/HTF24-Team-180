{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JvNLyturPCi",
        "outputId": "58de6f9e-61be-47f7-a684-7cc2de3d745b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCLXWIexrYk1",
        "outputId": "680695ff-1725-4aba-a6dd-5b663c154c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install seaborn\n",
        "%pip install matplotlib\n",
        "%pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bz8msZ7GmgHb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPueppijq2vG",
        "outputId": "616c6152-c696-4ae5-9e6b-afc433b7f2e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/hassaan/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/hassaan/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /home/hassaan/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /home/hassaan/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWkre9TQr6iU",
        "outputId": "2a458f79-d916-483b-9d4c-e87ec6c7d440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/hassaan/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DC3yP93eqw0k"
      },
      "outputs": [],
      "source": [
        "# Set to ignore warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional: Set the inline plotting for Jupyter Notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Visualize rating distribution\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Clean text function\n",
        "df = pd.read_csv('fake reviews dataset.csv')\n",
        "df.dropna(inplace=True)\n",
        "# Preload stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(review):\n",
        "    nopunc = [char for char in review if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    return ' '.join([word for word in nopunc.split() if word.lower() not in stop_words])\n",
        "\n",
        "\n",
        "# Apply text cleaning\n",
        "df['text_'] = df['text_'].astype(str)\n",
        "df['text_'] = df['text_'].apply(clean_text)\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Filter out stop words, digits, and punctuation\n",
        "    return ' '.join([word for word in tokens if word not in stop_words and not word.isdigit() and word not in string.punctuation])\n",
        "\n",
        "# Apply preprocessing in chunks\n",
        "chunk_size = 10000\n",
        "for i in range(0, df.shape[0], chunk_size):\n",
        "    df['text_'][i:i + chunk_size] = df['text_'][i:i + chunk_size].apply(preprocess)\n",
        "\n",
        "df['text_'] = df['text_'].str.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NDMS5wzpq8qH"
      },
      "outputs": [],
      "source": [
        "# Stemming and Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_words(text):\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "df['text_'] = df['text_'].apply(stem_words)\n",
        "df['text_'] = df['text_'].apply(lemmatize_words)\n",
        "\n",
        "# Save preprocessed dataset\n",
        "df.to_csv('Preprocessed Fake Reviews Detection Dataset.csv', index=False)\n",
        "\n",
        "df = pd.read_csv('Preprocessed Fake Reviews Detection Dataset.csv')\n",
        "\n",
        "def text_process(review):\n",
        "    nopunc = [char for char in review if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "df = df.dropna(subset=['text_'])\n",
        "df['text_'] = df['text_'].fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "74Lqjhw3HyjU"
      },
      "outputs": [],
      "source": [
        "bow_transformer = CountVectorizer(analyzer=text_process)\n",
        "\n",
        "bow_transformer.fit(df['text_'])\n",
        "\n",
        "review4 = df['text_'][3]\n",
        "\n",
        "bow_msg4 = bow_transformer.transform([review4])\n",
        "\n",
        "bow_reviews = bow_transformer.transform(df['text_'])\n",
        "\n",
        "tfidf_transformer = TfidfTransformer().fit(bow_reviews)\n",
        "tfidf_rev4 = tfidf_transformer.transform(bow_msg4)\n",
        "\n",
        "tfidf_reviews = tfidf_transformer.transform(bow_reviews)\n",
        "review_train, review_test, label_train, label_test = train_test_split(df['text_'],df['label'],test_size=0.35)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('bow',CountVectorizer(analyzer=text_process)),\n",
        "    ('tfidf',TfidfTransformer()),\n",
        "    ('classifier',SVC(probability=True))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctGG1vJDrCuW"
      },
      "outputs": [],
      "source": [
        "pipeline.fit(review_train,label_train)\n",
        "svc_pred = pipeline.predict(review_test)\n",
        "print('Model Prediction Accuracy:',str(np.round(accuracy_score(label_test,svc_pred)*100,2)) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGvXOi8qH7ol"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming `pipeline` is your trained model\n",
        "with open('fakeReviewClassifier.pkl', 'wb') as file:\n",
        "    pickle.dump(pipeline, file)\n",
        "\n",
        "print(\"Model saved to model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nC8SUJnZgGG-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: ['OR' 'OR' 'OR']\n",
            "Probability for CG: 26.44%\n",
            "Probability for OR: 73.56%\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "loaded_model = joblib.load('../models/fake_review_model.joblib')\n",
        "\n",
        "\n",
        "# Load the model from the pickle file\n",
        "# with open('../models/fake_review_model.pkl', 'rb') as file:\n",
        "#     loaded_model = pickle.load(file)\n",
        "\n",
        "# Test the loaded model with some data\n",
        "test_data = ['very bad product.','bad' , 'very bad noob']  # Replace with your actual test data\n",
        "predictions = loaded_model.predict(test_data)\n",
        "\n",
        "# Get the class labels and probabilities\n",
        "probabilities = loaded_model.predict_proba(test_data)\n",
        "class_labels = loaded_model.classes_\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Print the probabilities\n",
        "for label, prob in zip(class_labels, probabilities[0]):\n",
        "    print(f'Probability for {label}: {prob * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
